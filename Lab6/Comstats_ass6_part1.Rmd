---
title: "Comstats_ass6"
author: "Udaya Shanker Mohanan Nair, Dhanush Kumar Reddy Narayana Reddy"
date: "2025-03-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Implementation of 2 Assignment questions of Computational Statistics Lab 6 .

# Contributions

Member: Dhanush Kumar Reddy Narayana Reddy, Liu Id: dhana004, Contribution: Report writing and coding of question 1.

Member: Udaya Shanker Mohanan Nair, Liu Id: udamo524, Contribution: Report writing and coding of question 2.

# Question 1

## Part A

### Notation

- *n*: Number of observations.
- *x_j*: The *j*-th observation (*j* = 1, 2, ..., *n*).
- $K = 3$: Number of components in the mixture model.
- $\pi_k$: Mixing proportion for the *k*-th component (*k* = 1, 2, 3), where $\sum_{k=1}^{3} \pi_k = 1$.
- $\mu_k$: Mean of the *k*-th component.
- $\sigma_k$: Standard deviation of the *k*-th component.
- $\gamma_{jk}$: Responsibility of the *k*-th component for the *j*-th observation.

### E-Step (Expectation)

Compute the responsibilities $\gamma_{jk}$:

$$
\gamma_{jk} = \frac{\pi_k \cdot \mathcal{N}(x_j \mid \mu_k, \sigma_k)}{\sum_{l=1}^{3} \pi_l \cdot \mathcal{N}(x_j \mid \mu_l, \sigma_l)}
$$

where Probability Density Function (PDF) of Normal Distribution is:

$$
\mathcal{N}(x_j \mid \mu_k, \sigma_k) = \frac{1}{\sqrt{2\pi\sigma_k}} \exp\left( -\frac{(x_j - \mu_k)^2}{2\sigma_k^2} \right)
$$

### M-Step (Maximization)

Update the parameters $\pi_k$, $\mu_k$, and $\sigma_k$:

#### 1. Mixing Proportions:

$$
\pi_k = \frac{1}{n} \sum_{j=1}^{n} \gamma_{jk}
$$

#### 2. Means:

$$
\mu_k = \frac{\sum_{j=1}^{n} \gamma_{jk} \cdot x_j}{\sum_{j=1}^{n} \gamma_{jk}}
$$

#### 3. Variances:

$$
\sigma_k^2 = \frac{\sum_{j=1}^{n} \gamma_{jk} \cdot (x_j - \mu_k)^2}{\sum_{j=1}^{n} \gamma_{jk}}
$$

#### Standard Deviations

The standard deviation for component \( k \) is computed as:

$$
\sigma_k = \sqrt{ \frac{\sum_{j=1}^{n} \gamma_{jk} \cdot (x_j - \mu_k)^2}{\sum_{j=1}^{n} \gamma_{jk}} }
$$

## Part B

### Stopping Criterion

The algorithm stops when the change in the parameter vector \( \mathbf{pv} \) becomes smaller than a threshold \( \epsilon \). The scale-independent stopping criterion is:

$$
cc = \frac{\sqrt{\sum_{i=1}^{9} (pv_{i} - pv_{i,prev})^2}}{\sqrt{\sum_{i=1}^{9} pv_{i,prev}^2}} < \epsilon
$$
 Where:

- \( pv_i \): Current value of the \( i \)-th parameter.
- \( pv_{i,prev} \): Previous value of the \( i \)-th parameter.

This stopping Criterion is scale-independent, ensuring consistent behavior across different datasets.It measures relative change, making it sensitive to changes in all parameters, regardless of their magnitude.It is simple and computationally efficient to implement.

## Part C

```{r, echo=FALSE}
knitr::include_graphics("/Users/uday/Documents/GitHub/Computational_Statistics/Lab6/comstats_ass6_part1_c.jpeg")
```

Summary of Model Parameters:

|Parameter	        |  Component 1 |  Component 2 |	 Component 3 |
|----------------------------------------------------------------|
|Mixing Proportion  | 	0.2268454  |   0.2482586  |   0.524896   |
|Mean	              |   1.072835	 |   3.831919	  |   6.581551   | 
|Standard Deviation |	  1.198011	 |   0.717409	  |   0.6007124  |

The final parameter estimates correspond to the estimated parameters of the three-component normal mixture model, specifically:

Mixing Proportions:

These are the first three values: 0.2268454, 0.2482586, 0.524896. They represent the proportions of the data belonging to each of the three normal distributions.

Means: 

These are the next three values: 1.072835, 3.831919, 6.581551. They represent the estimated means of the three normal distributions.

Standard Deviations: 

These are the last three values: 1.198011, 0.717409, 0.6007124  . They represent the estimated standard deviations of the three normal distributions.

## Part D

```{r, echo=FALSE}
knitr::include_graphics("/Users/uday/Documents/GitHub/Computational_Statistics/Lab6/comstats_ass6_part1_d.jpeg")
```

The plots illustrate the evolution of each parameter estimate across iterations, indicating convergence. Specifically, the estimates for the mixing proportions (p1, p2, p3) stabilize after approximately 50–100 iterations, with only minor fluctuations in p2. The means (mu1, mu2, mu3) also show early fluctuations but generally stabilize around iterations 50–100, with mu3 remaining stable throughout, suggesting it reached convergence early. The standard deviations (sigma1, sigma2, sigma3) demonstrate similar behavior, with sigma1 decreasing but stabilizing after about 50 iterations and sigma2 and sigma3 reaching stability early. Overall, the plots suggest that all parameters have converged, as they exhibit stability and minimal fluctuations after a certain number of iterations. 

# Question 2

## Part A



## Part B



## Part C



# Appendix

## Question 1

```{r, echo=TRUE, eval=FALSE}
# Question: EM Algorithm
library(ggplot2)

# Part A
# EM Algorithm for 3 components
emalg_3_comp <- function(dat, eps = 0.000001) {
  n <- length(dat)
  pi1 <- rep(NA, n)  # initialize vector for prob. to belong to group 1
  pi2 <- rep(NA, n)  # initialize vector for prob. to belong to group 2
  pi3 <- rep(NA, n)  # initialize vector for prob. to belong to group 3
  
  # Define reasonable starting values for parameters
  p1 <- 1/3          # starting value for mixing parameter of group 1
  p2 <- 1/3          # starting value for mixing parameter of group 2
  p3 <- 1/3          # starting value for mixing parameter of group 3
  sigma1 <- sd(dat) * 2/3  # starting value for standard deviation in group 1
  sigma2 <- sigma1         # starting value for standard deviation in group 2
  sigma3 <- sigma1         # starting value for standard deviation in group 3
  mu1 <- mean(dat) - sigma1  # starting value for mean of group 1
  mu2 <- mean(dat)           # starting value for mean of group 2
  mu3 <- mean(dat) + sigma1  # starting value for mean of group 3
  pv <- c(p1, p2, p3, mu1, mu2, mu3, sigma1, sigma2, sigma3)  # parameter vector
  
  # Store parameter paras
  para <- list(p1 = c(), p2 = c(), p3 = c(), mu1 = c(), mu2 = c(), mu3 = c(), sigma1 = c(), sigma2 = c(), sigma3 = c())
  
  cc <- eps + 100  # initialize convergence criterion
  while (cc > eps) {
    pv1 <- pv  # save previous parameter vector
    
    ### E step ###
    for (j in 1:n) {
      pi1_j <- p1 * dnorm(dat[j], mean = mu1, sd = sigma1)
      pi2_j <- p2 * dnorm(dat[j], mean = mu2, sd = sigma2)
      pi3_j <- p3 * dnorm(dat[j], mean = mu3, sd = sigma3)
      total <- pi1_j + pi2_j + pi3_j
      pi1[j] <- pi1_j / total
      pi2[j] <- pi2_j / total
      pi3[j] <- pi3_j / total
    }
    
    ### M step ###
    p1 <- mean(pi1)
    p2 <- mean(pi2)
    p3 <- mean(pi3)
    mu1 <- sum(pi1 * dat) / (p1 * n)
    mu2 <- sum(pi2 * dat) / (p2 * n)
    mu3 <- sum(pi3 * dat) / (p3 * n)
    sigma1 <- sqrt(sum(pi1 * (dat - mu1)^2) / (p1 * n))
    sigma2 <- sqrt(sum(pi2 * (dat - mu2)^2) / (p2 * n))
    sigma3 <- sqrt(sum(pi3 * (dat - mu3)^2) / (p3 * n))
    
    pv <- c(p1, p2, p3, mu1, mu2, mu3, sigma1, sigma2, sigma3)
    
    # Part B
    cc <- sqrt(sum((pv - pv1)^2)) / sqrt(sum(pv1^2))  # scale-independent convergence criterion
    
    # Storing parameter values
    para$p1 <- c(para$p1, p1)
    para$p2 <- c(para$p2, p2)
    para$p3 <- c(para$p3, p3)
    para$mu1 <- c(para$mu1, mu1)
    para$mu2 <- c(para$mu2, mu2)
    para$mu3 <- c(para$mu3, mu3)
    para$sigma1 <- c(para$sigma1, sigma1)
    para$sigma2 <- c(para$sigma2, sigma2)
    para$sigma3 <- c(para$sigma3, sigma3)
  }
  
  # Part D
  par(mfrow = c(3, 3))
  for (param in names(para)) {
    plot(para[[param]], type = "l", main = param, xlab = "Iteration", ylab = "Value")
  }
  pv
}

# Data
load("C:/Users/dhanu/OneDrive/Documents/Computational Stats/Lab 6/threepops.Rdata")

# Part C
# Histogram
hist(dat3p, breaks = 20, main = "Histogram of dat3p", xlab = "Value")

# Fit the mixture model
results <- emalg_3_comp(dat3p)
cat("Final parameter estimates: \n",results)
```

## Question 2 

```{r, echo=TRUE, eval=FALSE}
Type you code here
```