---
title: "Lab2"
author: "Udaya Shanker Mohanan Nair, Dhanush Kumar Reddy Narayana Reddy"
date: "2025-02-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Statement of Contribution

Udaya Shanker Mohanan Nair (udma524) was mainly responsible to do Question 1. Dhanush Kumar Reddy Narayana Reddy (dhana004) was mainly responsible to do Question 2. The group report was compiled through discussion in a google document by everyone and the final draft was made using rmd.

## Question 1: Optimization of a two-dimensional function

### Part A

### Part B

### Part C

### Part D

### Part E

## Question 2: Maximum likelihood

Simple logistic regression:

$$
p(x) = P(Y = 1 | x) = \frac{1}{1 + \exp(-\beta_0 - \beta_1 x)}
$$

Log likelihood is:

$$
g(\mathbf{b}) = \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + \exp(-\beta_0 - \beta_1 x_i)} \right) + (1 - y_i) \log \left( 1 - \frac{1}{1 + \exp(-\beta_0 - \beta_1 x_i)} \right) \right]
$$

and the gradient is:

$$
g'(\mathbf{b}) = \sum_{i=1}^{n} \left\{ y_i - \frac{1}{1 + \exp(-\beta_0 - \beta_1 x_i)} \right\} 
\begin{bmatrix} 
1 \\ 
x_i 
\end{bmatrix}.
$$

### Part A

Using the steepest ascent method: 

Estimates: -0.009345799 1.262795

Function evaluations: 59

Gradient evaluations: 39

### Part B

Using the steepest ascent method for varient 1, where alpha = 1.0:

Estimates: -0.009345799 1.262795

Function evaluations: 59

Gradient evaluations: 39

Using the steepest ascent method for varient 2, where alpha = 1.5:

Estimates: -0.009351413 1.262783 

Function evaluations: 52 

Gradient evaluations: 28 

### Part C

For BFGS method:

Estimates: -0.009356126 1.262813 

Function evaluations: 12 

Gradient evaluations: 8

Using the BFGS method, the estimated values are \(\beta_0\) = -0.2 and \(\beta_1\) = 1, which are same as the initial starting values. This explains that the algorithm may not have significantly refined the estimates beyond the initial input. The precision appears to be constrained by the starting values, indicating that the optimization might not have fully converged to the maximum likelihood estimates.

The algorithm required 24 function evaluations and only 1 gradient evaluation. The minimal number of gradient evaluations shows that either the algorithm converged very quickly or the stopping criteria were met early, potentially limiting further iterations.

For Nelder-Mead method:

Estimates: -0.009423433 1.262738 

Function evaluations: 47 

Using the Nelder-Mead method, the estimated values are \(\beta_0\) = -0.009423433 and \(\beta_1\) = 1.262738, which differ from the initial starting values. This suggests that the algorithm has identified an alternative solution. The estimates are more precise, indicating a potentially more accurate result.

The algorithm required 47 function evaluations. Since Nelder-Mead does not rely on gradient computations, the higher number of function evaluations reflects a more thorough search for the optimal solution.

### Part D

For GLM:

Estimates: -0.009359853 1.262823

| Method                          | \(\beta_0\)  | \(\beta_1\) | Function Evaluations | Gradient Evaluations |
|---------------------------------|--------------|-------------|----------------------|----------------------|
| Steepest Ascent (alpha = 1.0)   | -0.009345799 | 1.262795    | 59                   | 39                   |
| Steepest Ascent (alpha = 1.5)   | -0.009351413 | 1.262783    | 52                   | 28                   |
| BFGS                            | -0.009356126 | 1.262813    | 12                   | 8                    |
| Nelder-Mead                     | -0.009423433 | 1.262738    | 47                   | NA                   |
| GLM                             | -0.009359853 | 1.262823    | -                    | -                    |


(1) The Steepest Ascent method (both variants), Nelder-Mead, and GLM yield similar estimates for \(\beta_0\) and \(\beta_1\), demonstrating consistency across these approaches. However, the BFGS method produces significantly different estimates, suggesting that it may not have properly converged or could be highly sensitive to the initial values.

(2) The Steepest Ascent method (both variants) requires the highest number of function evaluations due to its backtracking line search mechanism. Nelder-Mead also demands a substantial number of function evaluations but fewer than Steepest Ascent. BFGS, on the other hand, completes the optimization with the least number of function evaluations, highlighting its efficiency when it converges successfully.

(3) Since the Steepest Ascent method relies on gradient-based optimization, it requires multiple gradient evaluations throughout the process. In contrast, the BFGS method shows only one gradient evaluation, possibly due to rapid convergence. Nelder-Mead does not use gradients at all, resulting in an NA value for gradient evaluations.

(4) The estimates from Steepest Ascent (both variants), Nelder-Mead, and GLM are quite close, indicating high precision. However, BFGS appears to lack precision as it simply returns the starting values, hinting at potential convergence issues or sensitivity to initial parameter choices.

## Appendix

### Question 1 Code

```{r, eval=FALSE}
#Paste your code here.
```

### Question 2 Code

```{r, eval=FALSE}
# Question 2: Maximum likelihood
# Given Dosage Data
data <- data.frame(x = c(0, 0, 0, 0.1, 0.1, 0.3, 0.3, 0.9, 0.9, 0.9), 
                   y = c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1))

# Log-likelihood function
log_likelihood <- function(b, x, y) {
  beta0 <- b[1]
  beta1 <- b[2]
  p <- 1 / (1 + exp(-beta0 - beta1 * x))
  sum(y * log(p) + (1 - y) * log(1 - p))
}

# Gradient 
gradient <- function(b, x, y) {
  beta0 <- b[1]
  beta1 <- b[2]
  p <- 1 / (1 + exp(-beta0 - beta1 * x))
  grad_beta0 <- sum(y - p)
  grad_beta1 <- sum((y - p) * x)
  c(grad_beta0, grad_beta1)
}

# Steepest ascent with backtracking line search
steepest_ascent <- function(x, y, beta_values, alpha_value, error = 1e-5, max_iter = 1000) {
  b <- beta_values
  n_func_evals <- 0
  n_grad_evals <- 0
  
  for (i in 1:max_iter) {
    grad <- gradient(b, x, y)
    n_grad_evals <- n_grad_evals + 1
    
    # Backtracking line search
    alpha <- alpha_value
    while (TRUE) {
      b_new <- b + alpha * grad
      ll_new <- log_likelihood(b_new, x, y)
      n_func_evals <- n_func_evals + 1
      
      if (ll_new > log_likelihood(b, x, y) + 0.5 * alpha * sum(grad^2)) {
        break
      }
      alpha <- alpha * 0.5
    }
    
    # Check for convergence
    if (sqrt(sum((b_new - b)^2)) < error) {
      break
    }
    b <- b_new
  }
  
  list(estimates = b, n_func_evals = n_func_evals, n_grad_evals = n_grad_evals)
}

# Steepest ascent with alpha_value = 1.0
result_sa_va1 <- steepest_ascent(data$x, data$y, beta_values = c(-0.2, 1), alpha_value = 1.0)
print("Steepest Ascent (alpha_value = 1.0):")
print(result_sa_va1)

# Steepest ascent with alpha_value = 1.5
result_sa_va2 <- steepest_ascent(data$x, data$y, beta_values = c(-0.2, 1), alpha_value = 1.5)
print("Steepest Ascent (alpha_value = 1.5):")
print(result_sa_va2)

# Negative log-likelihood function for optim
neg_log_likelihood <- function(b, x, y) {
  beta0 <- b[1]
  beta1 <- b[2]
  p <- 1 / (1 + exp(-beta0 - beta1 * x))
  -sum(y * log(p) + (1 - y) * log(1 - p))
}

# Negative gradient of the log-likelihood
neg_gradient <- function(b, x, y) {
  beta0 <- b[1]
  beta1 <- b[2]
  p <- 1 / (1 + exp(-beta0 - beta1 * x))
  grad_beta0 <- -sum(y - p)
  grad_beta1 <- -sum((y - p) * x)
  c(grad_beta0, grad_beta1)
}

# BFGS
result_bfgs <- optim(par = c(-0.2, 1), fn = neg_log_likelihood, gr = neg_gradient, x = data$x, y = data$y, method = "BFGS")
print("BFGS Results:")
print(result_bfgs)

# Nelder-Mead
result_nm <- optim(par = c(-0.2, 1), fn = neg_log_likelihood, x = data$x, y = data$y, method = "Nelder-Mead")
print("Nelder-Mead Results:")
print(result_nm)

# logistic regression
model <- glm(y ~ x, data = data, family = binomial)
print("GLM Results:")
summary(model)

# Comparison
# Steepest Ascent for variant 1
cat("Estimates:", result_sa_va1$estimates, "\n")

cat("Function evaluations:", result_sa_va1$n_func_evals, "\n")

cat("Gradient evaluations:",  result_sa_va1$n_grad_evals, "\n")

# Steepest Ascent for variant 2
cat("Estimates:", result_sa_va2$estimates, "\n")

cat("Function evaluations:", result_sa_va2$n_func_evals, "\n")

cat("Gradient evaluations:",  result_sa_va2$n_grad_evals, "\n")

# BFGS
cat("Estimates:", result_bfgs$par, "\n")

cat("Function evaluations:", result_bfgs$counts[1], "\n")

cat("Gradient evaluations:", result_bfgs$counts[2], "\n")

# Nelder-Mead
cat("Estimates:", result_nm$par, "\n")

cat("Function evaluations:", result_nm$counts[1], "\n")

# GLM
cat("Estimates:", coef(model), "\n")
```